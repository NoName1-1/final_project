from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import json
import pandas as pd
from sqlalchemy import create_engine


def convert_seconds_to_time(seconds):
    """
    ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ ÑÐµÐºÑƒÐ½Ð´Ñ‹ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ HH:MM:SS.
    Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð¾ Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ "00:00:00".
    """
    try:
        seconds = pd.to_numeric(seconds, errors='coerce')
        if pd.isna(seconds):
            return "00:00:00"
        seconds = int(seconds) % 86400  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÐµÐ¼ 24 Ñ‡Ð°ÑÐ¾Ð²
        td = timedelta(seconds=seconds)
        return f"{td.seconds // 3600:02}:{(td.seconds % 3600) // 60:02}:{td.seconds % 60:02}"
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ hit_time: {e}")
        return "00:00:00"


# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… PostgreSQL
DB_CONN_STR = 'postgresql://project:project@host.docker.internal:5433/project'
engine = create_engine(DB_CONN_STR)

# ÐŸÑƒÑ‚Ð¸ Ðº JSON-Ñ„Ð°Ð¹Ð»Ð°Ð¼
JSON_DIR = '/opt/airflow/data/json_files'
PROCESSED_DIR = '/opt/airflow/data/processed_files'

# Ð¦ÐµÐ»ÐµÐ²Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ (event_value = 1)
TARGET_EVENTS = [
    'sub_car_claim_click', 'sub_car_claim_submit_click',
    'sub_open_dialog_click', 'sub_custom_question_submit_click',
    'sub_call_number_click', 'sub_callback_submit_click', 'sub_submit_success',
    'sub_car_request_submit_click'
]

# Ð‘Ñ€ÐµÐ½Ð´Ñ‹ Android Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ð¸ device_os
ANDROID_BRANDS = [
    'Huawei', 'Samsung', 'Xiaomi', 'Lenovo', 'Vivo', 'Meizu', 'OnePlus', 'BQ', 'Realme', 'OPPO', 'itel',
    'Nokia', 'Alcatel', 'LG', 'Tecno', 'Asus', 'Infinix', 'Sony', 'ZTE', 'Motorola', 'HTC', 'POCO'
]


def check_new_files(**kwargs):
    """
    ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ JSON-Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸.
    """
    files = [f for f in os.listdir(JSON_DIR) if f.endswith('.json')]
    if not files:
        raise ValueError("ÐÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… JSON-Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸.")
    return files  # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð²


def process_json_files(**kwargs):
    """
    ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ JSON-Ñ„Ð°Ð¹Ð»Ñ‹, Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² PostgreSQL.
    """
    ti = kwargs['ti']
    files = ti.xcom_pull(task_ids='check_files')

    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ session_id Ð¸Ð· Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    existing_sessions = pd.read_sql("SELECT session_id FROM sessions", engine)['session_id'].tolist()

    session_df_list = []
    hit_df_list = []

    for file in files:
        file_path = os.path.join(JSON_DIR, file)

        # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
        if os.stat(file_path).st_size == 0:
            print(f"ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»: {file}")
            continue

        with open(file_path, 'r') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² JSON-Ñ„Ð°Ð¹Ð»Ðµ: {file}")
                continue

        # Ð Ð°Ð·Ð±Ð¸Ñ€Ð°ÐµÐ¼ JSON (Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¸, ÐºÐ¾Ð³Ð´Ð° Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð°Ñ…Ð¾Ð´ÑÑ‚ÑÑ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ»ÑŽÑ‡Ð° Ð´Ð°Ñ‚Ñ‹)
        records = []
        for key, value in data.items():
            if isinstance(value, list):
                records.extend(value)
        df = pd.DataFrame(records)

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ðº ÐºÐ°ÐºÐ¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÑÑ JSON-Ñ„Ð°Ð¹Ð»
        if 'session_id' in df.columns and 'hit_date' in df.columns:
            hit_df_list.append(df)
        else:
            session_df_list.append(df)

    # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… sessions
    if session_df_list:
        df_sessions = pd.concat(session_df_list, ignore_index=True)
        df_sessions.drop_duplicates(subset='session_id', inplace=True)

        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÐµÑÑÐ¸Ð¸
        df_sessions = df_sessions[~df_sessions['session_id'].isin(existing_sessions)]

        if not df_sessions.empty:
            # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
            df_sessions.fillna({
                'utm_keyword': 'unknown',
                'utm_campaign': 'unknown',
                'utm_source': 'unknown',
                'utm_adcontent': 'unknown',
                'device_model': 'unknown',
                'device_os': 'other'
            }, inplace=True)

            # ÐšÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ device_os
            df_sessions.loc[df_sessions['device_os'] == '(not set)', 'device_os'] = 'unknown'
            df_sessions.loc[df_sessions['device_brand'] == 'Apple', 'device_os'] = 'ios'
            df_sessions.loc[df_sessions['device_brand'].isin(ANDROID_BRANDS), 'device_os'] = 'Android'
            df_sessions.loc[df_sessions['device_brand'] == 'Microsoft', 'device_os'] = 'Windows'
            df_sessions.loc[df_sessions['device_brand'] == 'BlackBerry', 'device_os'] = 'BlackBerry'
            df_sessions.loc[df_sessions['device_brand'] == 'Nokia', 'device_os'] = 'Windows Phone'
            df_sessions.loc[df_sessions['device_os'] == 'unknown', 'device_os'] = 'other'

            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð² PostgreSQL
            df_sessions.to_sql('sessions', engine, if_exists='append', index=False)
            print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð² sessions: {len(df_sessions)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")

    # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… hits
    if hit_df_list:
        df_hits = pd.concat(hit_df_list, ignore_index=True)
        df_hits.drop_duplicates(inplace=True)

        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ session_id, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐµÑÑ‚ÑŒ Ð² Ð±Ð°Ð·Ðµ
        df_hits = df_hits[df_hits['session_id'].isin(existing_sessions)]

        if not df_hits.empty:
            # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
            df_hits['event_label'] = df_hits['event_label'].fillna('unknown')
            df_hits['hit_referer'] = df_hits['hit_referer'].fillna('unknown')

            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ hit_time
            df_hits['hit_time'] = df_hits['hit_time'].astype(str).str.strip().replace('', '0')
            df_hits['hit_time'] = df_hits['hit_time'].apply(convert_seconds_to_time)

            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ
            df_hits['event_value'] = df_hits['event_action'].apply(lambda x: 1 if x in TARGET_EVENTS else 0)

            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð² PostgreSQL
            df_hits.to_sql('hits', engine, if_exists='append', index=False)
            print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð² hits: {len(df_hits)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")

    # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² Ð°Ñ€Ñ…Ð¸Ð²
    for file in files:
        os.rename(os.path.join(JSON_DIR, file), os.path.join(PROCESSED_DIR, file))
        print(f"ðŸ“‚ Ð¤Ð°Ð¹Ð» {file} Ð¿ÐµÑ€ÐµÐ¼ÐµÑ‰ÐµÐ½ Ð² {PROCESSED_DIR}")


# DAG Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ JSON-Ñ„Ð°Ð¹Ð»Ð¾Ð²
with DAG(
        'dag_json_processing',
        description='ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° JSON-Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð² PostgreSQL Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²',
        start_date=datetime(2025, 1, 20),
        schedule_interval='@daily',
        catchup=False
) as dag:
    check_files = PythonOperator(
        task_id='check_files',
        python_callable=check_new_files,
        provide_context=True
    )

    process_files = PythonOperator(
        task_id='process_files',
        python_callable=process_json_files,
        provide_context=True
    )

    check_files >> process_files
